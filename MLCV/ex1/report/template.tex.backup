\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath} 

\begin{document}

	\title{Assignment I\\MNIST Handwritten Digits Classification}
	\author{Adam Kosiorek}	
	\markboth{Machine Learning for Applications in Computer Vision}
	\maketitle
	


%\begin{abstract}
%  This report describes classification of handwritten digits from the MNIST dataset by an SVM classifier.
%\end{abstract}

\section{Objective}
    \PARstart{T}{he} first assignment was to explore the SVM classifier, Decision Trees and Random Forests for the MNIST handwritten digits recognition. 

\section{Dataset}
    The MNIST dataset is comprised of 60000 training and 10000 testing samples \cite{MNIST}. Each sample is a 28x28 pixel black and white image of a single centered digit. It is widly used as a benchmark for comparing machine learning algorithms.	

\section{Experiments}
    All experiments were carried out on a notebook with Intel i7-2670QM quad core CPU and 8Gb RAM with Python SciPy package.
    \subsection{Dataset Preparation}
	It is known that processing input data prior to classification can improve results. In case of MNIST and SVM, however, state of the arts results have been achieved without preprocessing, that is, with raw pixels \cite{VSVM}. I examine the influence of mean intensity subtraction and momentum-based deskewing on classification accuracy.
  
    \subsection{Classifier Tuning}
	Most classifiers can be fine-tuned to a particular task by adjusting their hyperparameters. They are rarely independent of each other and it is necessary to perform an exhaustive grid search in the hyperparameter space. I chose to do so with 10\% of the training data and 5-fold cross-validation. The parameters optimized for each classifier are shown in table \ref{tab:parameters}. 
	
		\begin{table}[!hbt]
		\begin{center}
		\caption{Parameters optimized for each classifier}
		\label{tab:parameters}
		\begin{tabular}{c|c}
			Classifier & Parameters \\
			\hline
			SVM & kernel type, C, gamma (for RBF), degree (for polynomial) \\
			\hline
			Decision Tree & cost function, max tree depth, number of features \\
			\hline
			Random Forests & number of estimators, cost function, \\ 
			& max estimator depth, number of features\\
		\end{tabular}
		\end{center}
	\end{table}
	
	Classifiers with the best performing parameter combinations were trained on the full dataset.
		
		
\section {Results and Discussion}
    Table \ref{tab:results} summarizes results. Mean subtraction and deskewing applied together delieverd the highest accuracy for SVM. Therefore, this composition was used in the rest of the experiments. Parameter tuning resulted in the following parameters for each classifier:
    \begin{itemize}
     \item SVM: \hfill polynomial kernel of degree 2, $C = 1000$
     \item Decision Tree: 500 max features, 100 max depth, entropy criterion
     \item Random Forest: 50 max features, 10 max depth, entripy criterion, 100 estimators
    \end{itemize}

	\begin{table}[!hbt]
		\begin{center}
		\caption{Results}
		\label{tab:results}
		\begin{tabular}{c|c|c|c}
			Classifier & Preprocessing & Accuracy \\
			\hline
			          & none             &            & 98.02\% \\
			SVM       & mean             & fine-tuned & 98.28\% \\
			          & deskew           &            & 98.70\% \\
			          & mean \& deskew   &            & 98.84\% \\
			\hline
			Decision & mean \& deskew    & default    & 90.44\% \\
			Tree     & mean \& deskew    & fine-tuned & 91.08\% \\
			\hline
			Random   & mean \& deskew    & default    & 96.01 \% \\
			Forest   & mean \& deskew    & fine-tuned & 96.28 \% \\
		\end{tabular}
		\end{center}
	\end{table}



% Now we need a bibliography:
\begin{thebibliography}{5}

	\bibitem{MNIST}
	Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner. ``Gradient-based learning applied to document recognition.'' Proceedings of the IEEE, 86(11):2278-2324, November 1998.
	
	\bibitem{MJH06} % Conference paper
	T.~Mayer, H.~Jenkac, and J.~Hagenauer. Turbo base-station cooperation for intercell interference cancellation. {\em IEEE Int. Conf. Commun. (ICC)}, Istanbul, Turkey, pp.~356--361, June 2006.
	
	\bibitem{VSVM}
	D.~Decoste and B.~Sch√∂lkopf. ``Training invariant support vector machines.'' Machine learning 46, no. 1-3 (2002): 161-190.

	\bibitem{Proakis} % Book
	J.~G.~Proakis. {\em Digital Communications}. McGraw-Hill Book Co.,
	New York, USA, 3rd edition, 1995.

	\bibitem{MNIST_Website} % Web document
	Y.~LeCun, C.~Cortes, C.J.C.~Burges. The MNIST database of handwritten digits.
	\url{http://yann.lecun.com/exdb/mnist/}.
	
	\bibitem{UFLDL} % Web document
	A.~Ng, J,~Ngiam, C.Y.~Foo, Y.~Mai and C~Suen. ``Unsupervised Feature Learning and Deep Learning Tutorial.''
	\url{http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial}.

	\bibitem{5}
	IEEE Transactions \LaTeX and Microsoft Word Style Files.
	\url{http://www.ieee.org/web/publications/authors/transjnl/index.html}

\end{thebibliography}

% Your document ends here!
\end{document}
