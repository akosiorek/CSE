2. Introduction - 3 min
  3) Show movie

  Imagine you enter a room and you start capturing your enviornment immediately. It is enough to swipe your leg and utter the label "floor" to learn the new class, which propages through all visible surface. Then you touch a chair and a new class is learned. Next comes the table. If an object is too small, you can encircle it. You add a cup class, too. 
  When you switch into the test phase, every object in the scene is classified with what you've just learned. If some of the predictions are wrong, like here, where a chair of different colour appears in the scene - no problem, just touch the missclasified surface and relabel it.
  As you can see, capturing your enviornment can be really easy and useful with applications in interior design, augmented reality or gathering datasets for large-scale computer vision challanges.

2. State of the Art - 5 min

  5) Scene Understading

  A lot of research has been done on scene labeling, object classification and detection and image segmentation on colour images, RGBD data, point clouds, meshes and volumetric representations. Most approaches are offline and certainly not real time with with Convolutional Neural Networks for object classification and detection, Conditional Random Fields  and global variational inference for segmentation.

  Valentin et. al. aggegates a sequence of depth data into volumetric TSDF representation, which is then triangulated and converted into a mesh. RGB features are computed from images and projected into the 3D model, while geometrical features are computed from the mesh directly. They achieve state of the art performance on KITTI and NYU datasets and claim that mesh speeds up and provides better results.

  Kim uses voxel-based CRF to model reconstruction and segmentation at the same time. Voxels cotain both visibility and occupancy information. Visibility is constrained such that a voxel is visble only if a ray from the camera can hit it; it is useful to model and reduce  noise in depth maps. This approach is off-line, since there is global inference that would have to be restarted if model changed.

  Herbst et. al. makes an attempt on online reconstruction and segmentation and allows model updates. The authors employ change detection to detect changes in models after updating them. Once a change is detected, the model is split in a static part (which hasn't changed) and a dynamic part (which has changed). It is online but not real-time with around 2s processing time per frame.


  6) Model-based SLAM

  SLAM is an important problem in robotics and one on which SemanticPaint realies heavily. A major contribution by Newcombe et. al., the KinectFusion, allows robust reconstruction of 3D enviornment from a single depth sensor. It uses model-based tracking to match an incoming depth frame against a reconstructed 3D model. Depth frames are fused into a global volume in the TSDF format. 

  Salas-Moreno et. al. extends KinectFusion with object classification capabilities. It uses a database of objects, which is built off line, to detect objects in the scene and builds a object-pose graph, which is a very sparse and compact representation of the world. It is online, but requires the database.

  Pradeep uses a single RGB camera to reconstruct 3D geometry. This works uses sparse tracking to estimate camera pose and then selects key frames and secondary frames from which stereo reconstruction is done. It works only for small scenes but for textured surfaces deliveres very good results, similar to KinectFusion.
  
3. Pipeline - 8 min

  8) Pipeline Overview

  So, how does it work? We take RGB and Depth data from Kinect and aggregate them into a TSDF volume. Then, we take user interactions - touch objects and labels and put that into Conditional Random Field, which puts priors on individual voxels. We also put that into a random forest classifier, which predicts classes of all objects that haven't been labeled. There's a feedback loop, since the forest is trained with voxel labels, which are then inferred by the mean-field inference algorithm using forest predictions. 

  9) VOPs

  The whole pipeline heavily depends on classification, and for that yo need discriminative features that are fast to compute if you want to do real time.. . That's why there's the VOP - Voxel Oriented Patch. 

  We take a square image patch centered at a voxel of interest and perpendicular to its normal. Here, we use a square of size 13 by 13 pixels and 10mm per pixel resolution. This patch stores colours from TSDF in CIELab format to handle illumination changes. Finally, we rotate the patch according to the dominant gradient direction for full 3D rotational invariance. Such feature, associated with every voxel, is used by the random forest classifiers. Speaking of which...

  10) RF

  Random Forests are really bagged classification trees, training in a greedy way on bootsreaped data in an off-line fashion, such that all data is needed at once. Finally, there's voting among trees for the final result. 

  11) SRF

  Streaming Random Forest are inherently on-line. Let's introduce some terminology. Let R_n be a reservoir at node n, initially there's only one at the roon note and let T_n be a list of samples stored in the reservoir. This list can hold at most K samples. When a new sample comes in, it is swapped with one of the samples in the reservoir, but the probability of such event decreases over time. The idea is that if we take samples over a huge time window, and that is what reservoir does, we'll have an unbiased sample of all data. 

  A node and a reservoir is split if the number of samples seen in a node is higher than N. Information gain is computed from the normalized class distribution at a given node.
  
  12) SRF - reservoir splitting

 When we create a node, we also create a distribution of split function. Now, when we want to split a node, we have to sweep through the reservoir for each split function, computing resulting class distributionand compute information gain. Now, rather than frow away statistics, we pass them to the child nodes which saves computation. 
  
  13) Dynamic CRF

  Conditional Random Fields are a subclass of Probabilistic Graphical Models. They are used mostly for classification of sequences, because they can capture (even global) contex. In Computer Vision, they are quite popular for segmentation. Here, it's used to smooth out the segmentation and to facilitate user interactions. 

  Each voxel's label is encoded as a random variable x_i, with likelihood psi_i and priors psi_ij. If we take negative log likelihood of the joint probability distribution, we get the labeling energy, which we'd like to minimize. Now, phi_i is a cost of assigning a label to a voxel and phi_ij are for smoothnes. Epsilon_i is a neighbourhood of the voxel i, here, a radius of 6 cm is used. 

  14) User Interactions

  Note that the energy landscape is changing all the time, one reason being that the potentials are updated for user interactions. So if we touch an object and say a label, we effectively set potentials for the touched voxels, to 0 if their label is correct and to infiniy otherwise.

 Encircling is a little more complicated. We take a convex hull of the selection as a foreground class and all the rest as the background class and fit a Gaussian Mixture Model. Then, we classify all voxels inside a bounding box around the selection and update the potentials. 

  15) Predictions and Smoothness

  Next come forest predictions. We update potentials for all otherwise unaffected voxels with a negative log likelihood of a their current class. To model smoothness, we update the pairwise potentials as a function of difference of position, appearence and normal directions of the voxels, with a couple of parameters to set by hand.

  16) Inference

  Then comes the Mean-Field inference. We approximate the original probability distribution P(x)  under KL-divergence by Q(x). Then, we use an iterative algorithm to refine the approximation. Here, since the energy landscape changes only gradually, we do only a single update in each frame. To speed up impact of the forest prediction, we initialize  each frame by a weighted sum of the last frame's result and the forest predictions.

4. Results - 2 min
  18) Segmentation

  Let's look at some segmentation results. Here, we have RGB volume, forest predictions and the inference result for two different scenes. We see that inference improves results quite a lot, for example here, and that we can also relabel some objects to correct mistakes.

  19)

  If we look at the percentage of correctly classified pixels, we see that user iteraction performs very well, but it affect only small regions. Forest predictions are also ok and they're significantly improved by the mean-field smoothing. 
  

  20) Features

  Next, let's look at the features. They're compared indirectly by comparing forest classification results with different features used for training and testing< which tells us a lot about their discriminative power. Here are the best three results and Voxel Oriented Patch delievers the most consistent results.

  21)

  Again, if we look at percentage, VOP is the best. Only the Kitchen scene is done better by the Depth Probe. It's due to very harsh lighting conditions there, which make RGB features inconclusive.   

  20) SRF

  Finally, Streaming Random Forest is compared against two other tree-based online classifiers, namely Online Random Forest and Hoefding Tree. To simulate an online setting, new classes and point of views are fed into the classifiers sequentially. Streaming Random Forests outperforms the other ones  quite comfortably. It's also computationally more efficient. Quite a sad fact is that the performance drops with the number of classes and handling complicated scenes becomes rather tricky.  


5. Discussion and Outlook - 2 min
  22) Summary

  To sum up, this work delivers a pipeline that lets you capture personalized 3D enviornments, exactly as you need them. It's fully interactive, requires no pre-training and works on-line. Possible applications include robot navigation, help to partially sighted people and capturing 3D datasets for large-scale computer vision challanges.

  I should probably add that the whole implementation, together with a thorough documentation is open-sourced.

  23) Failures

  Unfortunately, there are some failure cases. Bleeding, or misclassification at object edges, is caused by errors in depth and colour sensor alignment. Also strong illumination and viewpoint changes can lead to classification errors. 


  24) Future Work

  There is some future work to be done. Geometrical features could prevent bleeding and class priors (like walls are vertical) or priors for different enviornments could improve classification performance. There're also outdoor enviornment, that would require passive sensors. To handle bigger scenes with lots of classes, some research on better scalable algorithms is necessary.

6. Q&A? - 10 min

 Related Work:
 - what sparse tracking does Pradeep use?
 Pipeline
 - what is SDF? -> sensor data fusion
 - Why is CIELab used for VOPs? -> handle illumination changes, but why?
 - How is the dominan gradient direction chosen? -> SIFT, details
 - What are the colours? From TSDF, invalid colour value if empty voxel
 SRF
 - what kind of split functions is used? -> comparisons of random pixel itensities
 - how do reservoirs guarantee unbiased data? - taking samples from frames far apart on the time scale means we have non-correlated samples
 - why is data non-IID? -> subsequent frames are highly correlated
 CRF
 - how are smoothnes parameters chosen?

 Results: 
 - why is segmentation/results worse in the kitchen sequence?
 - What are Depth and Color Probes?
 - What is delta RGB mean?
 - Why is Depth Probe better in Kitchen?
 - How do ORF and HT differ from each other and from SRF?
