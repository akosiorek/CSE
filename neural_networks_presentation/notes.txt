1. Introduction

Neural Networks
Introduction and Best Practices

Adam Kosiorek
Feb 17, 2016

a.kosiorek@tum.de




Abstract:
	Neural Networks are cool. But why? What makes them so special? And why so late?
	We'll compare Neural Networks against standard Machine Learning approaches and 
	see how different architectures	help us model the problem at hand. Finally, 
	we'll give some training tips that actually enabled progress in the field.

Message Objective:
	Neural Networks are incredibly complex yet effective in learning. I would like to
	convice you to give them a try by showing you what cool things you can do with them,
	why they are so popular and what they really are.


Presentation:	

	1. Amazing Neural Networks - Motivation
		a) End to End Memory Networks
		b) Caption Generation
		c) Object Classification


	2. Why are Neural Networks so popular?
		a) Traditional ML
		* feature engineering as an essential part of every machine learning problem
		* expert knowledge, long time, high cost => nobody understands
		* further improvements almost impossible due to complexity
		* no clear rules for feature design
		* features independent of classifiers -> 
			to get an optimal performance you have to test multiple clfs

		b) Feature Learning in NNs
		* hierarchy of features
		* joint feature & classifier training
		* knowledge transfer
		

	3. What are Neural Networks and how do we train them? 
		a) Structure - Data Flow in a Computational Graph
		1) Feed Forward Neural Network
		- no neuron state; neuron activations in disjoint forward passes are independent
		- great for object recognition and other tasks where a single input's information is enough

		2) Recurrent Neural Networks 
		- introduce neuron state; activiation of a neuron depends on the current and all previous inputs
		- sequence learning: nlp e.g. machine translation, audio understanding, text generation

		b) Modeling - Cost Function & End to End Training
		
		
	4. Best Practices or What Took Us So Long?
		a) Lack of Computational Resources and Data
		b) Vanishing Gradients
		- pretraining		- weight initialization
		- activation functions
		c) Overfitting
		- dropout

Summary:
	We've just learned why Neural Networks are so cool, what they really are and how to use them 
	effectively. I hope I convinced you to give them a try. I guess most of you use Machine 
	Learning on a daily basis, right? Give neural networks a try and you won't be disappointed!

Questions:

	The question I get the most often is: Are NNs really so easy to use? Well, the answer is: no. 
	But neither is any of the machine learning algorithms out there. If you just throw your data 
	on an SVM and it works - good for you: you probably don't need anything better. If you want 
	to improve, however, it's not that easy. The drawback of NNs is that it won't work out of the 
	box. You have to give it some time before getting any results. Finally, it's usually worth it.


