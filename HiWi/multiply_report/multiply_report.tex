\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath} 
\usepackage{booktabs}
\usepackage{listings}

\begin{document}

	\title{Report on Implementation of Fast Multiplication Routines in Fortran}
	\author{B.Sc. Adam Kosiorek, M.Sc. Nils Zander}	
	\markboth{Institute for Computation in Engineering, Technische Universit\"at M\"unchen}{}
	\maketitle
	


\begin{abstract}
 This report describes the derivation of performance models and development of fast matrix multiplication routines in Fortran for Intel Xeon E5-2689 Sandy Bridge CPU for the ADHOC++ project.
\end{abstract}

\section{Introduction}
    \PARstart{T}{he} majority of numerically intensive software projects can be reduced on the lowest level to linear algebra problems. It is hardly surprising that the main suite of tools for them - Basic Linear Algebra Subroutines (BLAS) - is amongst the most heavily optimized software packages out there. Using advanced data distribution and load balancing techniques, it leverages data locality and various levels of parallelism to boost performance. Many BLAS implementations are, however, ill-suited for small sized problems. As shown later, Intel MKL~-- arguably the fastest implementation for x86 CPUs~\cite{MKL}~-- is easily outperformed by the most na\"ive general matrix-matrix multiplication code when inputs are not big enough. In many cases it is therefore possible and necessary to devise custom matrix multiplication routines optimized for smaller problems. We develop performance model and optimized code for the following cases of matrix-matrix multiplication:
    \begin{equation}
     C = Ax + C
    \end{equation}
    where $C \in \mathcal{R}^{N \times K}$, $A \in \mathcal{R}^{N \times M}$, $x \in \mathcal{R}^{M \times K}$ and $N \in \mathcal{N}^+$, $K \in \{1, 2, 3\}$


\section{Performance Model}
    Theoretical performance models, while not perfectly accurate, can hugely benefit development of optimized numerical programming routines by providing maximum performance estimates as a goal to strive for. Maximum theoretical performance can be computed by: 
    \begin{equation}
     P_{peak} = IPC * FPI * Frequency
    \end{equation}
    where $IPC$ - instructions per cycle, $FPI$ - floating point operations per instruction, $Frequency$ - CPU's frequency. Intel Xeon E5-2689 operates with the frequency of $2.9$ GHz and is capable of executing 1, 2 or 4 FPI, depending on the mode (Scalar, SSE, AVX) [need ref]. IPC depends on the numerical algorithm or the average number of floating point instructions executed per cycle to compute a single element of the output matrix. To derive IPC and $P_{peak}$, we refer to table \ref{tab:low_level}, which lists instructions that can be executed in one cycle by the Sandy Bridge CPU and to table \ref{tab:req}, which depicts number of each low level instructions needed to be executed to compute one entry of $C$ matrix. Finally, table \ref{tab:perf} summarizes results.
    
      \begin{table}[!hbt]
	\begin{center}
	  \caption{Low level instructions per cycle.}
	  \label{tab:low_level}
	  \begin{tabular}{c|c|c}
		  Instruction	& Scalar & AVX \\
		  \hline
		  load		& 1 or 2 & 1 \\
		  \hline
		  store		& 1 or 0 & 0.5 \\
		  \hline
		  multiply	& 1 & 1 \\
		  \hline
		  add 		& 1 & 1 \\
	  \end{tabular}
	\end{center}
      \end{table}
      
      \begin{equation}      
	  K=1:\;\;C(i, 1) = C(i, 1) + A(i, 1) * x(1, 1) 
      \end{equation} 
      
      \begin{equation}      
	\begin{split}
	  K=2:\;\;C(i, 1) = C(i, 1)  + A(i, 1) * x(1, 1) \\+ A(i, 2) * x(2, 1)
	\end{split}
      \end{equation}
      
      \begin{equation}      
	\begin{split}
	  K=3:\;\;C(i, 1) = C(i, 1) + A(i, 1) * x(1, 1) \\+ A(i, 2) * x(2, 1) \\+ A(i, 3) * x(3, 1)
	\end{split}
      \end{equation}
      
  \begin{table}[!hbt]
    \begin{center}
      \caption{Low level instructions required to compute one entry of $C$ matrix.}
      \label{tab:req}
      \begin{tabular}{c|c|c|c|c}
	K	& load & store & multiply & add \\
	\hline
	1	& 2 & 1 & 1 & 1 \\
	\hline
	2	& 3 & 1 & 2 & 2 \\
	\hline
	3	& 4 & 1 & 3 & 3
      \end{tabular}
    \end{center}
  \end{table}
 
  \begin{table}[!hbt]
    \begin{center}
      \caption{Floating point instructions per cycle and peak theoretical performance for Scalar and AVX modes.}
      \label{tab:perf}
      \begin{tabular}{c|c|c|c|c}
		& \multicolumn{2}{c}{IPC} & \multicolumn{2}{c}{$P_{peak}$ [GFlops]} \\
	K	& Scalar & AVX	& Scalar & AVX \\
	\hline
	1	& 1 & 1 & 2.9 & 11.60 \\
	\hline
	2	& 2 & 1.33 & 5.8 & 15.47 \\
	\hline
	3	& 2 & 1.5 & 5.8 & 17.40
      \end{tabular}
    \end{center}
  \end{table}


\section{Implementation and Results}

  Fortran is the programming language traditionally used to implement high performance numerical algorithms. One reason is that it makes weaker assumptions about memory layout of variables than e.g. C, thus making vectorization easier.  We decided to use Intel's Fortran compiler ifort, which often produces faster code than GNU's gfortran \cite{intel_comp}, but also provides easier to read and more complete optimization feedback. Performance maximization requires code vectorization or execution in AVX mode. The following subsections describe constraints and counter-measures that enabled successful vectorization.
  
  \subsection{Memory Layout}
    Fortran assumes column-major memory layout, that is, a single column of a matrix is comprised of elements adjacent in memory. In our case the routines are executed from C/C++, which assumes row-major memory layout. Normally, that would require declaration of transposed matrices in Fortran: e.g. double A[sizeA1][sizeA2] in C/C++ would become DOUBLE PRECISION A(sizeA2, sizeA1) in Fortran. Efficient multiplication, however, requires iteration along rows of the original matrices, which transfers to impossible to vectorize iteration along the innermost dimension in Fortran. Thus, we resort to computing indices manually and declaring matrices as one dimensional vectors i.e. DOUBLE~PRECISION~A(sizeA1~$*$~sizeA2).
    
    Another limiting factor is memory alignment. High performance AVX instructions can be used only when data are aligned at appropriate memory boundaries. Even if that is the case, the compiler cannot be sure if every possible input will be memory-aligned. One way to ensure it is to use inline compiler directives, in this case: !DEC\$ VECTOR ALIGNED just before a loop referencing vectors.
  
  \subsection{Data Dependency - Read after Write}
    Listing \ref{lst:raw} contains the Read after Write dependency. It prevents vectorization, since the same element in memory is referenced multiple times. One possible solution is to introduce a local variable that would accumulate the results and store it in C(i) only once as in listing \ref{lst:noraw}. Compilers than unrolls the outer loop containing code from listing \ref{lst:noraw} and vectorizes the result.
    \begin{lstlisting}[label=lst:raw, caption=Read after Write dependency.]
      DO i = 1, 3
	C(i) = C(i) + A(index + i) * x(i)
      END DO
    \end{lstlisting}
    
    \begin{lstlisting}[label=lst:noraw, caption=Read after Write dependency resolved.]
      sum = 0
      DO i = 1, 3
	sum = sum + A(index + i) * x(i)
      END DO
      C(i) = C(i) + sum
    \end{lstlisting}
    
   \subsection{Results}  
    Achieved results, summarized in table \ref{tab:results}, are not perfect and are true only for matrices small enough to fit in L2 cache. Divergence of theoretical and achieved results might stem from the fact that we did not consider memory bandwidth or cache effects.
  
  \begin{table}[!hbt]
    \begin{center}
      \caption{Achieved results and comparison with theoretical peak performance}
      \label{tab:results}
      \begin{tabular}{c|c|c}
	K	& $P_{achieved}$ [Gflops] & $P_{achieved} / P_{peak}$\\
	\hline
	1	& 9.17 & 0.79 \\
	\hline
	2	& 11.89 & 0.77 \\
	\hline
	3	& 13.96 & 0.8
      \end{tabular}
    \end{center}
  \end{table}

   


\begin{thebibliography}{5}
	
	\bibitem{MKL}
	Intel Math Kernel Library: \url{https://software.intel.com/en-us/intel-mkl}
	
	\bibitem{intel_comp}
	IntelÂ® Parallel Studio XE 2015 Product Brief: \url{https://goo.gl/vc5eYo}

\end{thebibliography}

% Your document ends here!
\end{document}
